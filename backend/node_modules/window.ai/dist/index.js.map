{"version":3,"sources":["../src/index.ts"],"sourcesContent":["// ChatML is a simple markup language for chat messages. More available here:\n// https://github.com/openai/openai-python/blob/main/chatml.md\nexport type ChatMessage = {\n  role: \"system\" | \"user\" | \"assistant\"\n  content: string\n}\n\nexport type ChatRole = ChatMessage[\"role\"]\n\nexport type PromptInput = {\n  prompt: string\n}\n\nexport type MessagesInput = {\n  messages: ChatMessage[]\n}\n\n// Input allows you to specify either a prompt string or a list of chat messages.\nexport type Input = PromptInput | MessagesInput\n\nexport function isMessagesInput(input: Input): input is MessagesInput {\n  return \"messages\" in input\n}\n\nexport type TextOutput = {\n  text: string\n}\n\nexport type MessageOutput = {\n  message: ChatMessage\n}\n\n// Output can be either a string or a chat message, depending on which Input type you use.\nexport type Output = TextOutput | MessageOutput\n\nexport function isTextOutput(output: Output): output is TextOutput {\n  return \"text\" in output\n}\n\nexport function isMessageOutput(output: Output): output is MessageOutput {\n  return \"message\" in output\n}\n\n// CompletionOptions allows you to specify options for the completion request.\nexport interface CompletionOptions<TModel> {\n  // If specified, partial updates will be streamed to this handler as they become available,\n  // and only the first partial update will be returned by the Promise.\n  onStreamResult?: (result: Output | null, error: string | null) => unknown\n  // What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n  // make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n  // Different models have different defaults.\n  temperature?: number\n  // How many completion choices to generate. Defaults to 1.\n  numOutputs?: number\n  // The maximum number of tokens to generate in the chat completion. Defaults to infinity, but the\n  // total length of input tokens and generated tokens is limited by the model's context length.\n  maxTokens?: number\n  // Sequences where the API will stop generating further tokens.\n  stopSequences?: string[]\n  // Identifier of the model to use. Defaults to the user's current model, but can be overridden here.\n  model?: TModel\n}\n\n// Error codes emitted by the extension API\nexport enum ErrorCode {\n  NotAuthenticated = \"NOT_AUTHENTICATED\",\n  PermissionDenied = \"PERMISSION_DENIED\",\n  RequestNotFound = \"REQUEST_NOT_FOUND\",\n  InvalidRequest = \"INVALID_REQUEST\",\n  ModelRejectedRequest = \"MODEL_REJECTED_REQUEST\"\n}\n\n// Event types emitted by the extension API\nexport enum EventType {\n  // Fired when the user's model is changed.\n  ModelChanged = \"model_changed\",\n  // Fired for errors\n  Error = \"error\"\n}\n\nexport type RequestID = string\n\nexport type EventListenerHandler<T> = (\n  event: EventType,\n  data: T | ErrorCode\n) => void\n\nexport const VALID_DOMAIN = \"https://windowai.io\" as const\n\nexport interface WindowAI<TModel = string> {\n  __window_ai_metadata__: {\n    domain: typeof VALID_DOMAIN\n    version: string\n  }\n\n  getCompletion(\n    input: Input,\n    options?: CompletionOptions<TModel>\n  ): Promise<Output | Output[]>\n\n  getCurrentModel(): Promise<TModel>\n\n  addEventListener<T>(handler: EventListenerHandler<T>): RequestID\n}\n\ndeclare global {\n  interface Window {\n    ai: WindowAI\n  }\n}\n\n// Checking against other window.ai implementations\nexport function hasWindowAI() {\n  return typeof globalThis.window.ai?.getCompletion === \"function\"\n\n  // Ref: https://github.com/alexanderatallah/window.ai/pull/34#discussion_r1170544209\n  // return (\n  //   !!globalThis.window.ai?.__window_ai_metadata__ &&\n  //   window.ai.__window_ai_metadata__.domain === VALID_DOMAIN\n  // )\n}\n\nconst DEFAULT_WAIT_OPTIONS = {\n  interval: 100,\n  timeout: 2_400 // https://github.com/alexanderatallah/window.ai/pull/34#discussion_r1170545022\n}\n\nexport async function waitForWindowAI(opts = DEFAULT_WAIT_OPTIONS) {\n  if (hasWindowAI()) {\n    return\n  }\n\n  await new Promise((resolve, reject) => {\n    let counter = 0\n    const timerInterval = setInterval(() => {\n      counter += opts.interval\n      if (counter > opts.timeout) {\n        clearInterval(timerInterval)\n        reject(new Error(\"window.ai not found\"))\n      }\n\n      if (hasWindowAI()) {\n        clearInterval(timerInterval)\n        resolve(true)\n      }\n    }, opts.interval)\n  })\n}\n\nexport const getWindowAI = async (opts = DEFAULT_WAIT_OPTIONS) => {\n  // wait until the window.ai object is available\n  await waitForWindowAI(opts)\n  return globalThis.window.ai\n}\n"],"mappings":";AAoBO,SAAS,gBAAgB,OAAsC;AACpE,SAAO,cAAc;AACvB;AAaO,SAAS,aAAa,QAAsC;AACjE,SAAO,UAAU;AACnB;AAEO,SAAS,gBAAgB,QAAyC;AACvE,SAAO,aAAa;AACtB;AAuBO,IAAK,YAAL,kBAAKA,eAAL;AACL,EAAAA,WAAA,sBAAmB;AACnB,EAAAA,WAAA,sBAAmB;AACnB,EAAAA,WAAA,qBAAkB;AAClB,EAAAA,WAAA,oBAAiB;AACjB,EAAAA,WAAA,0BAAuB;AALb,SAAAA;AAAA,GAAA;AASL,IAAK,YAAL,kBAAKC,eAAL;AAEL,EAAAA,WAAA,kBAAe;AAEf,EAAAA,WAAA,WAAQ;AAJE,SAAAA;AAAA,GAAA;AAcL,IAAM,eAAe;AAyBrB,SAAS,cAAc;AAC5B,SAAO,OAAO,WAAW,OAAO,IAAI,kBAAkB;AAOxD;AAEA,IAAM,uBAAuB;AAAA,EAC3B,UAAU;AAAA,EACV,SAAS;AAAA;AACX;AAEA,eAAsB,gBAAgB,OAAO,sBAAsB;AACjE,MAAI,YAAY,GAAG;AACjB;AAAA,EACF;AAEA,QAAM,IAAI,QAAQ,CAAC,SAAS,WAAW;AACrC,QAAI,UAAU;AACd,UAAM,gBAAgB,YAAY,MAAM;AACtC,iBAAW,KAAK;AAChB,UAAI,UAAU,KAAK,SAAS;AAC1B,sBAAc,aAAa;AAC3B,eAAO,IAAI,MAAM,qBAAqB,CAAC;AAAA,MACzC;AAEA,UAAI,YAAY,GAAG;AACjB,sBAAc,aAAa;AAC3B,gBAAQ,IAAI;AAAA,MACd;AAAA,IACF,GAAG,KAAK,QAAQ;AAAA,EAClB,CAAC;AACH;AAEO,IAAM,cAAc,OAAO,OAAO,yBAAyB;AAEhE,QAAM,gBAAgB,IAAI;AAC1B,SAAO,WAAW,OAAO;AAC3B;","names":["ErrorCode","EventType"]}